{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Выполняем весь код с семинара для получения корпуса\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/st/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield simple_preprocess(str(sentence), deacc=True)\n",
    "\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    if allowed_postags is None:\n",
    "        allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качаем Mallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Запускаем вот такую функцию сначала в широком диапазоне [10,30), потом сужаем до [15,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# max_coh = 0\n",
    "# best_groups = 0\n",
    "#\n",
    "# for i in range(10,30):\n",
    "#     ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=i, id2word=id2word)\n",
    "#     coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "#     coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "#     print(i, coherence_ldamallet)\n",
    "#     if max_coh < coherence_ldamallet:\n",
    "#         max_coh = coherence_ldamallet\n",
    "#         best_groups = i\n",
    "#\n",
    "# print()\n",
    "# print(max_coh, best_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем в первом случае число групп 23, во втором 21. Во втором прогоне было больше, чем в первом, так что берем 21\n",
    "\n",
    "На самом деле, значения на прогонах разные даже для одного и того же числа групп, максимум наблюдается на промежутке от 19 до 23 где-то.\n",
    "\n",
    "Далее получаем топики по генсим-модели, проходим по всем текстам и для каждого в словаре считаем суммы весов слов из топиков, если они встречаются.\n",
    "Ключ с максимальным значением в таком словаре - широкий топик для текста.\n",
    "Тексты по широким топикам тоже кидаем в общий словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=21,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "import operator\n",
    "\n",
    "def findTextCommonTopic(text, all_topics=topics):\n",
    "    text_topics = {}\n",
    "    for word in text:\n",
    "        for topic in all_topics:\n",
    "            for topic_word in topic[1]:\n",
    "                if word == topic_word[0]:\n",
    "                    if topic[0] in text_topics:\n",
    "                        text_topics[topic[0]]+=topic_word[1]\n",
    "                    else:\n",
    "                        text_topics[topic[0]]=topic_word[1]\n",
    "    if text_topics:\n",
    "        return max(text_topics.items(), key=operator.itemgetter(1))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "groups = {}\n",
    "\n",
    "for text in texts:\n",
    "    text_topic = findTextCommonTopic(text)\n",
    "    if text_topic:\n",
    "        if text_topic in groups:\n",
    "            groups[text_topic].append(\" \".join([word for word in text]))\n",
    "        else:\n",
    "            groups[text_topic] = [\" \".join([word for word in text])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ищем TF-IDF в cловах для каждой группы, сохраняем топ5 для текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def processGroup(group_topic, group):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(group)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    denseList = vectors.todense().tolist()\n",
    "    tfidfs = {}\n",
    "    data = []\n",
    "    for i in range(len(group)):\n",
    "        for num, word in enumerate(feature_names):\n",
    "            tfidfs[word] = denseList[i][num]\n",
    "        top5 = sorted(tfidfs, key=tfidfs.get, reverse=True)[:5]\n",
    "        data.append([group[i], group_topic, top5])\n",
    "    return data\n",
    "\n",
    "\n",
    "data = []\n",
    "for group_topic in groups:\n",
    "    data.extend(processGroup(group_topic, groups[group_topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим массив, в котором сложим слова топиков по порядку наших данных.\n",
    "Сложим это все в датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topic_words = [topic[1] for row in data for topic in topics if row[1] == topic[0]]\n",
    "\n",
    "dataframe = pd.DataFrame(data, columns=['Text', 'Topic_id', 'top5'])\n",
    "dataframe.insert(2, 'topic words', topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Text  Topic_id  \\\n0      where thing car nntp_poste host park line wond...        15   \n1      si poll final summary final call si clock repo...        15   \n2      question engineering computer network distribu...        15   \n3      division line host amber write write article k...        15   \n4      man tumor treatment thank people respond reque...        15   \n...                                                  ...       ...   \n11282  horse breeding sale dear sir private agricultu...         9   \n11283  write look program algorithm use computation l...        13   \n11284  sale sale machine condition scratch fully oper...        13   \n11285  kick write question teach long time ago really...        13   \n11286  shall whole law word sin restriction kent writ...        13   \n\n                                             topic words  \\\n0      [(line, 0.2247845), (host, 0.09429961), (nntp_...   \n1      [(line, 0.2247845), (host, 0.09429961), (nntp_...   \n2      [(line, 0.2247845), (host, 0.09429961), (nntp_...   \n3      [(line, 0.2247845), (host, 0.09429961), (nntp_...   \n4      [(line, 0.2247845), (host, 0.09429961), (nntp_...   \n...                                                  ...   \n11282  [(number, 0.022235667), (may, 0.02095375), (al...   \n11283  [(sorry, 0.0945596), (utility, 0.04175744), (b...   \n11284  [(sorry, 0.0945596), (utility, 0.04175744), (b...   \n11285  [(sorry, 0.0945596), (utility, 0.04175744), (b...   \n11286  [(sorry, 0.0945596), (utility, 0.04175744), (b...   \n\n                                                  top5  \n0                    [car, door, lerxst, funky, where]  \n1                    [clock, si, poll, upgrade, final]  \n2               [display, machine, bunch, hear, store]  \n3       [division, quadrilateral, weitek, chip, amber]  \n4      [treatment, astrocytoma, bouncing, prob, tumor]  \n...                                                ...  \n11282            [horse, firm, class, breed, breeding]  \n11283                  [day, length, time, long, able]  \n11284           [ask, manual, sale, scratch, shipping]  \n11285               [angel, dragon, fight, hurl, kick]  \n11286      [love, metaphysical, apparently, book, law]  \n\n[11287 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Topic_id</th>\n      <th>topic words</th>\n      <th>top5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>where thing car nntp_poste host park line wond...</td>\n      <td>15</td>\n      <td>[(line, 0.2247845), (host, 0.09429961), (nntp_...</td>\n      <td>[car, door, lerxst, funky, where]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>si poll final summary final call si clock repo...</td>\n      <td>15</td>\n      <td>[(line, 0.2247845), (host, 0.09429961), (nntp_...</td>\n      <td>[clock, si, poll, upgrade, final]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>question engineering computer network distribu...</td>\n      <td>15</td>\n      <td>[(line, 0.2247845), (host, 0.09429961), (nntp_...</td>\n      <td>[display, machine, bunch, hear, store]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>division line host amber write write article k...</td>\n      <td>15</td>\n      <td>[(line, 0.2247845), (host, 0.09429961), (nntp_...</td>\n      <td>[division, quadrilateral, weitek, chip, amber]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>man tumor treatment thank people respond reque...</td>\n      <td>15</td>\n      <td>[(line, 0.2247845), (host, 0.09429961), (nntp_...</td>\n      <td>[treatment, astrocytoma, bouncing, prob, tumor]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11282</th>\n      <td>horse breeding sale dear sir private agricultu...</td>\n      <td>9</td>\n      <td>[(number, 0.022235667), (may, 0.02095375), (al...</td>\n      <td>[horse, firm, class, breed, breeding]</td>\n    </tr>\n    <tr>\n      <th>11283</th>\n      <td>write look program algorithm use computation l...</td>\n      <td>13</td>\n      <td>[(sorry, 0.0945596), (utility, 0.04175744), (b...</td>\n      <td>[day, length, time, long, able]</td>\n    </tr>\n    <tr>\n      <th>11284</th>\n      <td>sale sale machine condition scratch fully oper...</td>\n      <td>13</td>\n      <td>[(sorry, 0.0945596), (utility, 0.04175744), (b...</td>\n      <td>[ask, manual, sale, scratch, shipping]</td>\n    </tr>\n    <tr>\n      <th>11285</th>\n      <td>kick write question teach long time ago really...</td>\n      <td>13</td>\n      <td>[(sorry, 0.0945596), (utility, 0.04175744), (b...</td>\n      <td>[angel, dragon, fight, hurl, kick]</td>\n    </tr>\n    <tr>\n      <th>11286</th>\n      <td>shall whole law word sin restriction kent writ...</td>\n      <td>13</td>\n      <td>[(sorry, 0.0945596), (utility, 0.04175744), (b...</td>\n      <td>[love, metaphysical, apparently, book, law]</td>\n    </tr>\n  </tbody>\n</table>\n<p>11287 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S.\n",
    "Coherence определяет, насколько действительно семантически близки слова внутри одного топика"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}